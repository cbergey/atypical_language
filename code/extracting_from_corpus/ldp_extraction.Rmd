---
title: "UD Pipe"
author: "Dan Yurovsky"
date: '`r Sys.Date()`'
output:
  html_document:
  highlight: tango
theme: sandstone
code_folding: show
toc: false
toc_float: false
---

```{r libraries}
#library(cleanNLP)
library(udpipe)
#library(reticulate)
library(feather)
library(here)
library(tidyverse)
library(glue)


use_python("/Users/dan/.pyenv/shims/python")

#cnlp_init_corenlp(lang = "en")

source(here("read_ldp.R"))
```

```{r udpipe-switchboard, message = FALSE}
switchboard <- read_feather(here("data/switchboard.feather")) %>%
  mutate(doc_id = 1:n()) %>%
  rename(text = value)

udmodel <- udpipe_load_model(file = "english-ewt-ud-2.4-190531.udpipe")

switch_parses <- switchboard %>%
    udpipe(., udmodel, parallel.cores = 4) %>%
    as_tibble() %>%
    mutate(doc_id = as.numeric(doc_id)) %>%
    left_join(switchboard, by = c("doc_id", "sentence" = "text"))
  
write_feather(switch_parses, here(glue("data/switchboard_parses.feather")))

```

```{r run-udpipe, message = FALSE, eval = FALSE}
run_udpipe <- function(filename) {
  utterances <- read_csv(here(glue("data/{filename}.txt"))) %>%
    rename(doc_id = id, text = utts)

  udmodel <- udpipe_load_model(file = "english-ewt-ud-2.4-190531.udpipe")

  parses <- utterances %>%
    udpipe(., udmodel, parallel.cores = 4) %>%
    as_tibble() %>%
    mutate(doc_id = as.numeric(doc_id)) %>%
    left_join(utterances, by = c("doc_id", "sentence" = "text"))
  
  write_feather(parses, here(glue("data/{filename}_parses.feather")))
}

walk(c("ldp_child_original", "ldp_parent_original"), run_udpipe)
```

```{r get-pairs-udpipe}
get_pairs <- function(filename) {
  
  parses <- read_feather(here(glue("data/{filename}_parses.feather")))

  adj_parses <- parses %>%
    group_by(doc_id) %>%
    filter(any(upos == "ADJ"))
  
  #Fixes: 
  #1. adj whose dep_rel is "conj" modify adjectives that modify nouns. Make 
  # these modify the nouns directly.
  #2. nouns that are compounds we think are really adjectives.
  #3. adjs that modify ROOT modify something else in the sentence. 
  # note those so that we can hand-code (head_token_id == 0)
  compound_parses <- parses %>%
    group_by(doc_id) %>%
    filter(any(upos == "NOUN" && dep_rel == "compound"))
  
  adjs <- adj_parses %>%
    filter(upos == "ADJ") %>%
    select(doc_id, sentence, any_of("chat"), token_id, token, head_token_id, lemma, upos,
           dep_rel) %>%
    rename(adj_token = token, adj_token_id = token_id, adj_pos = upos,
           adj_lemma = lemma)
  # 
  # conj_targets <- adjs %>%
  #   filter(dep_rel != "conj") %>%
  #   select(doc_id, sentence, adj_token, adj_token_id, head_token_id) %>%
  #   rename(new_dep = adj_token, new_head_id = head_token_id)

  nouns_setup <- adj_parses %>%
    select(doc_id, sentence, any_of("chat"), token_id, token, lemma, upos, dep_rel,
           head_token_id)
  
    
  #fix conj adjs - don't do
  # conjs <- adjs %>%
  #   filter(dep_rel == "conj") %>%
  #   left_join(conj_targets, by = c("doc_id", "sentence", 
  #                                  "head_token_id" = "adj_token_id")) %>%
  #   filter(!is.na(new_head_id), new_head_id != 0) %>%
  #   select(-head_token_id, -dep_rel, -new_dep) %>%
  #   rename(head_token_id = new_head_id)

  #fix compounds
  compounds <- nouns_setup %>%
    filter(dep_rel == "compound") %>%
    rename(adj_token_id = token_id, adj_lemma = lemma, adj_token = token,
           adj_pos = upos)
  
  # get root-modifying ADJS
  root_adjs <- adj_parses %>%
    group_by(doc_id) %>%
    filter(any(upos == "NOUN")) %>%
    mutate(noun_count = sum(upos == "NOUN"),
           pron_count = sum(upos == "PRON")) %>%
    ungroup() %>%
    filter(upos == "ADJ" | upos == "NOUN")
  
  single_noun_roots <- root_adjs %>%
    filter(upos == "NOUN", noun_count == 1, pron_count == 0) %>%
    rename(noun_token_id = token_id, noun_lemma = lemma, noun_token = token) %>%
    select(doc_id, sentence, noun_token_id, noun_lemma, noun_token)
  
  root_adjs_processed <- root_adjs %>%
    filter(upos == "ADJ") %>%
    select(doc_id, paragraph_id, sentence, any_of("chat"), token_id, token, head_token_id,
           lemma, upos, dep_rel) %>%
    rename(adj_token = token, adj_token_id = token_id, adj_pos = upos,
           adj_lemma = lemma) %>%
    filter(head_token_id == 0, str_detect(sentence, " ")) %>%
    left_join(single_noun_roots, by = c("doc_id", "sentence")) %>% 
    mutate(noun_token_id = if_else(is.na(noun_token_id), 
                                   head_token_id, noun_token_id),
           noun_token = if_else(is.na(noun_token), "ROOT", noun_token),
           noun_lemma = if_else(is.na(noun_lemma), "ROOT", noun_lemma),
           adj_token_id = as.numeric(adj_token_id),
           noun_token_id = as.numeric(noun_token_id),
           prenominal = if_else(noun_token_id == 0, NA, 
                                (noun_token_id - adj_token_id) == 1)) %>%
    select(doc_id, sentence, adj_token_id, adj_token, noun_token_id,noun_token, 
           adj_lemma, noun_lemma, prenominal)
  
  # Note: Cases where adj_token != adj_lemma should be hand-checked
  pairs <- adjs %>%
    #bind_rows(conjs) %>% # add fixed conjunctions
    bind_rows(compounds) %>% # add compounds
    left_join(nouns_setup %>% select(-head_token_id), 
              by = c("doc_id", "sentence",
                                 "head_token_id"="token_id")) %>%
    rename(noun_pos = upos, noun_token_id = head_token_id, noun_token = token,
           noun_lemma = lemma) %>%
    filter(noun_pos == "NOUN") %>%
    select(doc_id, adj_token_id, noun_token_id, sentence, any_of("chat"), adj_token, 
           noun_token, adj_lemma, noun_lemma) %>%
    mutate_at(vars(adj_token_id, noun_token_id), as.numeric) %>%
    mutate(prenominal = adj_token_id == noun_token_id - 1) %>%
    bind_rows(root_adjs_processed) %>% # add ROOT adjs
    filter(!adj_token == "um", !noun_token == "one") %>%
    mutate(adj_lower = str_to_lower(adj_token)) %>%
    select(-adj_lemma)
  #   
  # pairs %>%
  #   filter(noun_token == "ROOT") %>%
  #   select(doc_id, sentence, adj_lower, noun_token) %>%
  #   sample_frac(1) %>%
  #   write_csv(here("data/root_adjs_to_code.csv"))
  # 
  # noncompounds <- nouns_setup %>%
  #   filter(dep_rel != "compound", upos == "NOUN") %>%
  #   select(doc_id, sentence, any_of("chat"), token, lemma) %>%
  #   rename(noun_token = token, noun_lemma = lemma)
  #   
  # possible_pairs <- adjs %>%
  #    bind_rows(compounds) %>%
  #    select(doc_id, sentence, any_of("chat"), adj_token, adj_lemma) %>%
  #    left_join(noncompounds, by = c("doc_id", "sentence", "chat"))
  # 
  write_csv(pairs, here(glue("data/{filename}_parsed_pairs_compounds_and_root.csv")))
  
#  write_csv(possible_pairs, here(glue("data/{filename}_parsed_pairs_all_pairs.csv")))
}

walk(c("ldp_parent_original", "ldp_child_original"), get_pairs)

#get_pairs("switchboard")
```

```{r run-corenlp, message = FALSE, eval = FALSE}
run_corenlp <- function(filename) {
  utterances <- read_lines(here(glue("data/{filename}.txt"))) %>%
    enframe(name = NULL, value = "text") %>%
    mutate(doc_id = 1:n())

  parses <- utterances %>%
    cnlp_annotate(., verbose = 10000) %>%
    first() %>%
    left_join(utterances, by = "doc_id")

  write_feather(parses, here(glue("data/{filename}_parses_corenlp.feather")))
}

walk(c("ldp_child_original", "ldp_parent_original"), run_corenlp)
```

```{r get-pairs-corenlp}
get_pairs <- function(filename) {
  
  parses <- read_feather(here(glue("data/{filename}_parses_corenlp.feather"))) %>%
    rename(token_id = tid, head_token_id = tid_source, sentence = text)

  adj_parses <- parses %>%
    group_by(doc_id) %>%
    filter(any(upos == "ADJ"))
  
  adjs <- adj_parses %>%
    filter(upos == "ADJ") %>%
    select(doc_id, sentence, token_id, token, head_token_id, lemma, upos) %>%
    rename(adj_token = token, adj_token_id = token_id, adj_pos = upos,
           adj_lemma = lemma)
  
  nouns_setup <- adj_parses %>%
    select(doc_id, sentence, token_id, token, lemma, upos)
  
  # Note: Cases where adj_token != adj_lemma should be hand-checked
  pairs <- adjs %>%
    left_join(nouns_setup, by = c("doc_id", "sentence", 
                                 "head_token_id"="token_id")) %>%
    rename(noun_pos = upos, noun_token_id = head_token_id, noun_token = token,
           noun_lemma = lemma) %>%
    filter(noun_pos == "NOUN") %>%
    select(doc_id, adj_token_id, noun_token_id, sentence, adj_token, 
           noun_token, adj_lemma, noun_lemma) %>%
    mutate_at(vars(adj_token_id, noun_token_id), as.numeric) %>%
    mutate(prenominal = adj_token_id == noun_token_id - 1)
  
  write_csv(pairs, here(glue("data/{filename}_parsed_pairs_corenlp.csv")))
}

walk(c("ldp_parent_original", "ldp_child_original"), get_pairs)
```

```{r}
pairs <- read_csv(here("data/ldp_parent_original_parsed_pairs.csv"))

View(pairs)
```

