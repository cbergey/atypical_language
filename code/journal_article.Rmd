---
title             : "Language selectively encodes atypical features of the world"
shorttitle: "Language encodes the atypical"

# author: 
#   - name          : "Claire Augusta Bergey*"
#     affiliation   : "1"
#     corresponding : yes    # Define only one corresponding author
#     address       : "450 Jane Stanford Way, Building 420, Stanford, CA 94305"
#     email         : "cbergey@stanford.edu"
#   - name          : "Ben Morris*"
#     affiliation   : "2"
#     email         : "benjamin.morris@yale.edu"
#   - name          : "Dan Yurovsky"
#     affiliation   : "3"
# 
# affiliation:
#   - id            : "1"
#     institution   : "Stanford University"
#   - id            : "2"
#     institution   : "Yale University"
#   - id            : "3"
#     institution   : "Carnegie Mellon University"

author: 
   - name          : "Anonymous Authors"
     affiliation   : "1"

affiliation:
   - id            : "1"
     institution   : "Institution Anonymized for Review"
 

abstract: |
  Language contains a wealth of information about the world. However, language does not necessarily reflect the world veridically; instead, communicative pressure may lead it to selectively encode surprising or atypical information. If language picks out the atypical features of things (e.g., “purple carrot”) more often than the typical features of things (e.g., “orange carrot”), learning about the world from language is not straightforward. Here, we test whether a bias to overrepresent atypical information is present and robust across a variety of sources: everyday conversations among adults, the language children hear from parents, and children’s own language.  To do so, we extracted usage data for nearly 5,000 unique adjective-noun pairs and collected human typicality ratings for each pair. We found adults speaking to other adults, parents speaking to children, and even children themselves predominantly use adjectives to mark atypical features of things. We also found that parents of very young children comment on typical features slightly more often than parents of older children. Thus, language is structured to emphasize what is atypical—so how can one learn about what things are typically like from language? Using large language models, we test how this bias shapes what can be learned from language alone.  We find that even language models with extensive training data (word2vec and BERT) fail to capture the typicality of adjective–noun pairs well, and only a much more sophisticated large language model (GPT-3) succeeds. Though large language models have input unlike what human learners have access to, they provide useful bounds on the typicality information learnable from applying simple training objectives to language alone. In sum, we find that people talk more about the atypical than the typical, and we examine how this shapes the problem of learning about the world from language in children, adults, and language models.
  

keywords          : language input, language acquisition, child-directed speech, corpus analysis, language models

bibliography      : ["purple-carrots.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"    
output            : papaja::apa6_pdf
---


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo = F, warning = F, cache = F, 
                      message = F, sanitize = T)
# Note: to build, 
options(digits=2)
```

```{r, libraries}
library(png)
library(ggplot2)
library(here)
library(ggridges)
library(scales)
library(tidyboot)
library(xtable)
library(papaja)
library(lme4)
library(lmerTest)
library(ggthemes)
library(broom.mixed)
library(weights)
library(tidyverse)
library(grid)
library(udpipe)
library(ggh4x)
library(ggpubr)
library(ggh4x)

theme_set(theme_few())
```
Does language reflect the world? A strong correspondence between the world and language undergirds current theories of language and concept learning across a variety of domains. Children's early word learning is thought to proceed largely through dependable associations between language and sensory percepts (e.g., hearing "cup" and seeing a cup at the same time) and words with other conceptually related words (e.g., associating "cup" and "bowl" after hearing them together in an utterance) [@woodward1994rapid; @smith2008infants; @sloutsky2004induction; @unger2020statistical; @savic2022exposure]. Congenitally blind children and adults learn visual concepts that are similar to those of their sighted peers, presumably primarily through language [@landau2009; @bedny2019; @kim_knowledge_2019]. Further, language models' broad success in approximating human judgments across a variety of domains suggests that language supplies a lot of information about the world [@landauer1997; @mikolov2013; @devlin2018; @brown_language_2020].

In this paper, we argue that language in fact systematically departs from reflecting the world by selectively picking out remarkable facets of it. We rarely use language to provide running commentary on the world around us; instead, we use language to talk about things that diverge from our expectations or those of our conversational partner [@grice1975; @sperber1986relevance; @clark1990pragmatics; @rohde2021s]. For instance, in lab tasks, people often mention the color of a brown banana but let the color of a yellow banana go unmentioned [@westerbeek2015; @rubio-fernandez2016]. Given the communicative pressure to be informative, naturalistic language statistics may provide surprisingly little evidence about what is typical: we may rarely hear that a banana is yellow. Here, we show that this pressure pervasively structures naturalistic language use—among adults, from adults to children, and by children—and complicates the problem faced by children and computational models when learning about the world from language.

To investigate whether people tend to mention the atypical, we first examined the typicality of adjectives with respect to the nouns they describe in a large corpus of adults' naturalistic conversation. We show that people's tendency to mention atypical features, as observed in constrained lab tasks, pervasively structures language use in a corpus of adults' conversations: people more often mention the atypical than the typical features of things. 

```{r utt_table, results="asis", tab.env = "table"}
tab <- tibble(utterance = c("especially with wooden shoes.",
                            "you like red onions?", 
                            "the garbage is dirty."),
              pair = c("wooden-shoe", "red-onion", "dirty-garbage"),
              `rating 1` = c(2, 5, 7),
              `rating 2` = c(2, 3, 6),
              `rating 3` = c(2, 4, 6),
              `mean` = c(2, 3.6, 6)) %>% 
  xtable(display = c("s", "s", "s", "d", "d", "d", "f"),
         caption = "Sample typicality ratings from three human coders for three adjective-noun pairs drawn from the corpus. Ratings are on a scale from 1 (never) to 7 (always). Note that means may be slightly different from the mean of the three ratings shown here because some pairs have more than three ratings.",
         label = "tab:utt_table")

print(tab, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
      floating.environment = "table",
      include.rownames = FALSE)
```
We next examine whether parents, too, talk predominantly about the atypical features of things. If parents speak to children the way they speak to other adults, children may be faced with input that emphasizes atypicality in relation to world knowledge they do not yet have. On the other hand, parents may speak to children far differently from the way they speak to other adults: parents may calibrate their language to children's limited world knowledge (the Linguistic Tuning Hypothesis, see @snow1972; @leung2021), and thus speech to children may reflect the typical features of the world more veridically. In a large corpus of parent-child interactions recorded in children's homes, we find that parents overwhelmingly choose to mention atypical rather than typical features and limited evidence of calibration; further, we find that children themselves mention more atypical than typical features. 

We then ask whether the co-occurrence structure of language nonetheless captures typicality information by testing whether the distributional semantics model word2vec captures adjective-noun typicality. We find that relatively little typical feature information is represented in these semantic spaces. We also test whether two more advanced language models, BERT and GPT-3, capture typicality, and find that only the latter does well. These models are unlikely to reflect children's learning mechanisms or language input, but tell us what kinds of typicality information are learnable from language in principle.

# Part I: People remark on the atypical

## Method

In order to determine whether people use adjectives mostly to mark atypical features of categories, we analyzed speech from large corpora of everyday conversations: adult-adult conversations, caregivers' speech to children, and children's own speech. We extracted adjectives and the nouns they modified from conversational speech, and asked a sample of Amazon Mechanical Turkers to judge how typical the property described by each adjective was for the noun it modified. We then examined both the broad features of this typicality distribution and the way it changes over development.^[Our typicality elicitation method, and analyses and predicted results regarding child-directed speech were pre-registered at the following link: https://osf.io/g4c7r/?view_only=0b46742f44174d09b5c4dd790197e18f . This pre-registration specifies a prior version of our method for extracting adjective-noun pairs; results from the exact pre-registered analyses are available in a proceedings paper (redacted for blind review) and conform to our pre-registered predictions. The analyses in the present manuscript use an improved method for extracting adjective-noun pairs, and conform to those same predictions. The corpus, analysis plan, and predictions about child-directed speech did not change from the first pre-registration. The fact that the same hypotheses are borne out under both extraction methods demonstrates that these findings are robust to these data processing decisions.]

### Corpora

For adult-adult speech, we used data from the Conversation Analytic British National Corpus, a corpus of naturalistic, informal conversations in people's everyday lives [@albert_cabnc_2015; @coleman_audio_2012]. We excluded any conversations with child participants, for a total of 99,305 adult-adult utterances.

For our child-directed and child-produced speech, we used data from the Language Development Project, a large-scale, longitudinal corpus of parent-child interactions recorded in children's homes. Families were recruited to be representative of the Chicagoland area in both socio-economic and racial composition; all families spoke English at home [@goldin-meadow2014]. Recordings were taken in the home every 4 months from when the child was 14 months old until they were 58 months old, resulting in 12 timepoints. Each recording was of a 90-minute session in which parents and children were free to behave and interact as they liked. Our sample consisted of 64 typically developing children and their caregivers with data from at least 4 timepoints (*mean* = 11.3 timepoints). Together, this resulted in a total of 641,402 parent utterances and 368,348 child utterances.

### Stimulus Selection

We parsed each utterance in our corpora using UDPipe, an automated dependency parser, and extracted adjectives and the nouns they modified. This set contained a number of abstract or evaluative adjective-noun pairs whose typicality would be difficult to classify (e.g., "good"--"job"; "little"--"bit"). To resolve this issue, we used human judgments of words' concreteness to identify and exclude non-concrete adjectives and nouns [@brysbaert2014]. From concreteness ratings of almost 40,000 concepts, we selected only the concepts with average concreteness ratings in the top 25% (more than 9,000 concepts), which excluded concepts with a mean concreteness ratings less than 3.90 out of 5 [@brysbaert2014].  We retained for analysis only pairs in which both the adjective and noun were in the top 25% of concreteness ratings (e.g., “slippery” – “balloon”), excluding pairs below that threshold (e.g., “thin” – “strip”). Additionally, we further excluded pairs that included a particular adjective that escaped our concreteness filtering–“bloody”–which was identified as highly concrete in our concreteness norms (that are based on the American English usage), but should be excluded given that British English speakers (like those in the CABNC corpus) use it abstractly and evaluatively.

Our final sample included 6,370 unique adjective-noun pairs drawn from 7,471 parent utterances, 2,775 child utterances, and 1,867 adult-adult utterances. The pairs were combinations of 1,498 distinct concrete nouns and 1,388 distinct concrete adjectives. We compiled these pairs and collected human judgments on Amazon Mechanical Turk for each pair, as described below. Table \ref{tab:utt_table} contains example utterances from the final set and typicality judgments from our human raters.

## Participants

Each participant rated 35 adjective-noun pairs, and we aimed for each pair to be rated five times, for a total of 910 rating tasks. Participants were allowed to rate more than one set of pairs and were paid $0.80 per task. Distribution of pairs was balanced using a MongoDB database that tracked how often sets of pairs had been rated. If a participant allowed their task to expire with the task partially complete, we included those ratings and re-recruited the task. Overall, participants completed 32,461 ratings. After exclusions using an attention check that asked participants to simply choose a specific number on the scale, we retained 32,293 judgments, with each adjective–noun pair retaining at least two judgments.

## Design and Procedure

To evaluate the typicality of the adjective–noun pairs that appeared in parents' speech, we asked participants on Amazon Mechanical Turk to rate each pair. Participants were presented with a question of the form "How common is it for a cow to be a brown cow?" and asked to provide a rating on a seven-point scale: (1) never, (2) rarely, (3) sometimes, (4) about half the time, (5) often, (6) almost always, (7) always. We also gave participants the option to select "Doesn't make sense" if they could not understand what the adjective-noun pair would mean. Pairs that were marked with "Doesn't make sense" by two or more participants were excluded from the final set of pairs: 1,591 pairs were excluded at this stage, for a final set of 4,779 rated adjective-noun pairs. Some of these nonsense pairs likely resulted from imperfect automated part of speech tagging (e.g., till—dinner, wipe—face); others were unorthodox uses of description or difficult to imagine out of context (e.g., back—mom, square—circle, teeth—show). Though there are many of these nonsense exclusions, this criterion is conservative and likely errs on the side of excluding atypical pairs rather than typical ones.

```{r read-data-judgments}
coded_data <- read_csv(here("../data/ldp_cabnc_data.csv"))

all_subjs <- coded_data %>%
  count(subid)

keep_subjs <- coded_data %>%
  filter(is.na(adjective) & is.na(noun) & rating == 5)

subj_data <- coded_data %>%
  filter(!(is.na(adjective) & is.na(noun))) %>%
  filter(subid %in% keep_subjs$subid) 

incomplete_subs <- coded_data %>%
  group_by(subid) %>%
  summarise(ntrials = n()) %>%
  filter(ntrials != 36)

nonsense_pairs <- subj_data %>%
  group_by(adjective, noun) %>%
  tally(rating == 8) %>%
  filter(n >= 2) %>%
  mutate(adj_noun_phrase = paste(adjective, noun, sep = " "))

subj_data <- subj_data %>%
  mutate(adj_noun_phrase = paste(adjective, noun, sep = " ")) %>%
  filter(!(adj_noun_phrase %in% nonsense_pairs$adj_noun_phrase),
         rating != 8)

mean_ratings <- subj_data %>%
  group_by(adjective, noun) %>%
  summarise(mean_typ = mean(rating)) %>%
  ungroup()
```

```{r read-corpora}
all_corpora <- read_csv(here("../data/all_corpora_adj_noun_utts_redacted.csv"))

orig_tokens <- read_csv(here("../data/ldp_cabnc_pairs_all_info.csv"))

ldp <- all_corpora %>%
  filter(dataset != "cabnc_utts_udpipe")

all_corpus_counts <- all_corpora %>%
  left_join(orig_tokens, by = c("adj_token" = "adj", "noun_token" = "noun_orig")) %>%
  rename(adjective = adj_token) %>%
  count(adjective, noun) %>%
  ungroup()

cabnc <- all_corpora %>%
  filter(dataset == "cabnc_utts_udpipe") %>%
  left_join(orig_tokens, by = c("adj_token" = "adj", "noun_token" = "noun_orig")) %>%
  rename(adjective = adj_token) %>%
  select(doc_id, noun_token, adjective, noun, prenominal, 
         dataset, mass, adj_article, 
         article, adj_token_id, noun_token_id) %>%
  left_join(mean_ratings, by = c("adjective", "noun")) %>%
  rename(mean_rating = mean_typ) %>%
  filter(!is.na(mean_rating))


# NOTE: actual utterances from LDP are redacted for data sharing 
  # to preserve anonymity while sharing as much as possible
ldp_coded <- all_corpora %>%
  filter(dataset %in% c("ldp_parent_utts_udpipe", "ldp_child_utts_udpipe")) %>%
  left_join(orig_tokens, by = c("adj_token" = "adj", "noun_token" = "noun_orig")) %>%
  mutate(adjective = adj_token) %>%
  select(doc_id, noun_token, adjective, noun, prenominal, 
         dataset, mass, adj_article,adj_token, 
         article, adj_token_id, noun_token_id, subject, session, person) %>%
  left_join(mean_ratings, by = c("adjective", "noun")) %>%
  rename(mean_rating = mean_typ) %>%
  mutate(age = (4*session + 10)) %>%
  filter(!is.na(mean_rating))
```

```{r read-judgments}
all_pairs <- subj_data %>%
  distinct(adjective, noun, article, adj_article)

#write_csv(all_pairs, here("../data/final_pairs_ldp_cabnc.csv"))

# then we run that file through word2vec/get_wiki_similarities.py to get this file ...
w2v_judgments <- read_csv(here("../data/w2v_sims_ldp_cabnc.csv")) %>%
  mutate(
         wiki_similarity = as.numeric(wiki_similarity),
         ldp_similarity = as.numeric(ldp_similarity))

bert_judgments <- read_csv(here("../data/bert_judgments_ldp_cabnc.csv")) %>%
  select(adjective, noun, prob, is_multi_token)

gpt3_judgments_raw <- read_csv(here("../data/gpt3_judgments_ldp_cabnc.csv")) %>%
  rbind(read_csv(here("../data/gpt3_judgments_ldp_cabnc_2.csv"))) %>%
  rbind(read_csv(here("../data/gpt3_judgments_ldp_cabnc_3.csv")))

gpt3_judgments <- gpt3_judgments_raw %>%
  mutate(gpt3_judgment = str_remove(tolower(gpt3_judgment), "[.]")) %>%
  mutate(gpt3_judgment = if_else(gpt3_judgment == "usually rarely", "rarely", gpt3_judgment)) %>%
  mutate(gpt3_score = case_when(gpt3_judgment == "never" ~ 1,
                                gpt3_judgment == "rarely" ~ 2,
                                gpt3_judgment == "sometimes" ~ 3,
                                gpt3_judgment == "about half the time" ~ 4,
                                gpt3_judgment == "often" ~ 5,
                                gpt3_judgment == "almost always" ~ 6,
                                gpt3_judgment == "always" ~ 7))

all_judgments <- mean_ratings %>%
  left_join(bert_judgments, by = c("adjective", "noun")) %>%
  rename(bert_prob = prob) %>%
  left_join(w2v_judgments, by = c("adjective", "noun")) %>%
  left_join(gpt3_judgments, by = c("adjective", "noun")) %>%
  mutate(wiki_similarity = if_else(is.nan(wiki_similarity), NA_real_, wiki_similarity)) %>%
  mutate(ldp_similarity = if_else(is.nan(ldp_similarity), NA_real_, ldp_similarity))
```

```{r models}
parent_tokens <- ldp_coded %>%
  filter(person == "parent") %>%
  count(session, age, adjective, noun) %>%
  left_join(mean_ratings) %>%
  filter(!is.na(mean_typ)) %>%
  mutate(centered_rating = mean_typ - 4)

kid_tokens <- ldp_coded %>%
  filter(person == "child") %>%
  count(session, age, adjective, noun) %>%
  left_join(mean_ratings) %>%
  filter(!is.na(mean_typ)) %>%
  mutate(centered_rating = mean_typ - 4)

cabnc_tokens <- cabnc %>%
  count(adjective, noun) %>%
  left_join(mean_ratings) %>%
  filter(!is.na(mean_typ)) %>%
  mutate(centered_rating = mean_typ - 4)

mean_parent_type <- lmer(centered_rating ~ 1 + (1|noun),
                     data = parent_tokens) %>%
                     tidy() %>%
                     filter(effect == "fixed")

mean_parent_type_byage <- parent_tokens %>%
  group_by(age) %>%
  nest() %>%
  mutate(effect = map(data, ~lmer(centered_rating ~ 1 + (1|noun),
                     data = .) %>%
                       tidy() %>%
                       filter(effect == "fixed"))) %>%
  select(-data) %>%
  unnest(cols = c(effect)) %>%
  ungroup()

mean_parent_token <- lmer(centered_rating ~ 1 + (1|noun), weights = n,
                     data = parent_tokens) %>%
                     tidy() %>%
                     filter(effect == "fixed")

mean_parent_token_byage <- parent_tokens %>%
  group_by(age) %>%
  nest() %>%
  mutate(effect = map(data, ~lmer(centered_rating ~ 1 + (1|noun),
                                  weights = n, data = .) %>%
                       tidy() %>%
                       filter(effect == "fixed"))) %>%
  select(-data) %>%
  unnest(cols = c(effect)) %>%
  ungroup()


parent_token_weight <- lmer(centered_rating ~ log(age) + (1|noun),
                     weights = n,
                     data = parent_tokens) %>%
                     tidy() %>%
                     filter(effect == "fixed")


parent_type_weight <- lmer(centered_rating ~ log(age) + (1|noun),
                     data = parent_tokens) %>%
                     tidy() %>%
                     filter(effect == "fixed")


mean_cabnc_token <- lmer(centered_rating ~ 1 + (1|noun), weights = n, data  = cabnc_tokens) %>%
                     tidy() %>%
                     filter(effect == "fixed")

mean_kid_token <- lmer(centered_rating ~ 1 + (1|noun), weights = n, data  = kid_tokens) %>%
                     tidy() %>%
                     filter(effect == "fixed")
```
```{r result-effects}
token_estimate <- mean_parent_token %>% pull(estimate)
token_statistic <- mean_parent_token %>% pull(statistic)
token_p <- mean_parent_token %>% pull(p.value) %>% printp()

token_age_estimate <- mean_parent_token_byage %>% slice(1) %>% pull(estimate)
token_age_statistic <- mean_parent_token_byage %>% slice(1) %>% pull(statistic)
token_age_p <- mean_parent_token_byage %>% slice(1) %>% pull(p.value) %>% printp()

cabnc_token_estimate <- mean_cabnc_token %>% pull(estimate)
cabnc_token_statistic <- mean_cabnc_token %>% pull(statistic)
cabnc_token_p <- mean_cabnc_token %>% pull(p.value) %>% printp()

token_development_estimate <- parent_token_weight %>% filter(term == "log(age)") %>% 
  pull(estimate)
token_development_statistic <- parent_token_weight %>% filter(term == "log(age)") %>% 
  pull(statistic)
token_development_p <-parent_token_weight %>% filter(term == "log(age)") %>% 
  pull(p.value) %>% printp()

kid_token_estimate <- mean_kid_token %>% filter(term == "(Intercept)") %>%
  pull(estimate)
kid_token_statistic <- mean_kid_token %>% filter(term == "(Intercept)") %>%
  pull(statistic)
kid_token_p <- mean_kid_token %>% filter(term == "(Intercept)") %>%
  pull(p.value) %>% printp()
```

### Results

We combined the human typicality ratings with usage data from our corpora to examine the extent to which parents, children, and adults speaking to other adults use language to describe typical and atypical features. In our analyses, we token-weighted these judgments, giving higher weight to pairs that occurred more frequently in speech. However, results are qualitatively identical and all significant effects remain significant when examined on a type level.

First, we examine whether adults speaking to other adults in naturalistic conversation talk about atypical features more than typical ones. Examining adjective-noun usage in the Conversation Analytic British National Corpus, we found that adult-adult speech predominantly features atypical adjective-pairs (Figure \ref{fig:cabnc-parent-overall}). To confirm this effect statistically, we centered the ratings (i.e. "about half" was coded as 0), and then predicted the rating on each trial with a mixed effects model with only an intercept and a random effect of noun (\texttt{typicality $\sim$ 1 + (1|noun)}). The intercept was reliably negative, indicating that adult-adult speech more often points out atypical than typical features ($\beta =$ `r cabnc_token_estimate`, $t =$ `r cabnc_token_statistic`, $p$ `r cabnc_token_p`). 

Though adults highlight atypical features when talking to other adults, they may speak differently when talking to children. If caregivers speak informatively to convey what is atypical or surprising in relation to their own sophisticated world knowledge, we should see that caregiver description is dominated by adjectives that are sometimes or rarely true of the noun they modify. If instead child-directed speech privileges seemingly redundant information, perhaps calibrating to young children's limited world knowledge, caregiver description should yield a distinct distribution dominated by highly typical modifiers. Examining adjective-noun use in the LDP, we found that caregivers' description predominantly focuses on features that are atypical (Figure \ref{fig:distribution-plot}).

```{r cabnc-parent-overall, fig.align = "center", fig.height = 4, fig.width = 6, fig.cap = "Density plots showing use of atypical and typical adjective-noun pairs by parents speaking to children and adults speaking to other adults.", fig.pos="tb"}
ldp_coded %>%
  filter(person == "parent") %>%
  mutate(corpus = "Parent speech") %>%
  select(adjective, noun, mean_rating, corpus) %>%
  bind_rows(cabnc %>% mutate(corpus = "Adult-adult speech") %>% 
              select(adjective, noun, mean_rating, corpus)) %>%
  mutate(typicality=mean_rating) %>%
  ggplot(aes(x = typicality)) +
  geom_density(fill="cornsilk") +
  xlab("More Atypical                   More Typical \n Typicality of adjective-noun pairs") +
  geom_vline(xintercept = 4, size=1, linetype="solid")+
  facet_wrap(~corpus, ncol = 1) +
  theme_few() +
  theme(#panel.grid = element_line(color="lightgrey",size=0.5), 
    axis.line = element_line(colour = "black"),
    axis.ticks = element_line(),
    axis.text.x = element_text(size=11, angle=28, hjust=1),
    axis.text.y = element_text(size=11),
    legend.position = "none") +
  scale_x_continuous(minor_breaks = seq(1 , 7, 1), breaks = seq(1, 7, 1), labels = c('never', 'rarely', 'sometimes', 'about half', 'often', 'almost always', 'always')) +
  scale_y_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4)) +
  theme(
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)

```

```{r distribution-plot, fig.align = "center", fig.width=3.4, fig.height = 5.4, fig.cap = "Density plots showing parents' use of atypical and typical adjective-noun pairs across their child's age.", fig.pos="tb"}
ldp_coded %>%
  filter(person == "parent") %>%
  mutate(typicality=mean_rating) %>%
  group_by(session) %>%
  mutate(age = min(age)) %>%
  ggplot(aes(x = typicality, y=age, group=age, fill=age)) +
  geom_density_ridges2() +
  ylab("Child Age (months)") +
  xlab("More Atypical                   More Typical \n Typicality of adjective-noun pairs") +
  geom_vline(xintercept = 4, size=1, linetype="solid")+
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme_few() +
  theme(#panel.grid = element_line(color="lightgrey",size=0.5), 
    axis.line = element_line(colour = "black"),
    axis.ticks = element_line(),
    axis.text.x = element_text(size=11, angle=28, hjust=1),
    axis.text.y = element_text(size=11),
    legend.position = "none") +
  scale_x_continuous(minor_breaks = seq(1 , 7, 1), breaks = seq(1, 7, 1), labels = c('never', 'rarely', 'sometimes', 'about half', 'often', 'almost always', 'always')) +
  scale_y_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4)) +
  theme(
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)
```

We confirmed this effect statistically using the same model structure as above, finding a reliably negative intercept that indicates more atypical than typical adjective-noun pairs ($\beta =$ `r token_estimate`, $t =$ `r token_statistic`, $p$ `r token_p`). We then re-estimated these models separately for each age in the corpus, and found a reliably negative intercept for every age group (smallest effect $\beta_{14-month-olds} =$ `r token_age_estimate`, $t =$ `r token_age_statistic`, $p$ `r token_age_p`). Even when talking with very young children, caregiver speech is structured according to the kind of communicative pressures observed in adult-adult conversation.

<!-- Examining usage data as a function of typicality (see Figure \ref{fig:distribution_plot}), we see evidence of a positive skew (0.65). Data from every time point from 14-58 months seems to show a similar pattern (skews 0.23 - 0.82). These skews provide further evidence that the the bulk of caregiver language reflects lower-typicality adjective-noun pairs. -->

```{r compute_prototypicals}
parent_typical_ratings <- ldp_coded %>%
  filter(person == "parent") %>%
  group_by(age, adjective, noun, mean_rating) %>%
  count() %>%
  ungroup() %>%
  mutate(typical = mean_rating >= 5)

typicals_parent <- ldp_coded %>%
  filter(person == "parent") %>%
  group_by(age, adjective, noun, mean_rating) %>%
  count() %>%
  ungroup() %>%
  mutate(typical = mean_rating >= 5) %>%
  group_by(age, typical) %>%
  summarise(weighted_sum = sum(n), sum = n()) %>%
  pivot_longer(cols = c(sum, weighted_sum), names_to = "measure", values_to = "sum") %>%
  group_by(age, measure) %>%
  mutate(prop = sum / sum(sum)) %>%
  filter(typical)

typicals_cabnc <- cabnc %>%
  group_by(adjective, noun, mean_rating) %>%
  count() %>%
  ungroup() %>%
  mutate(typical = mean_rating >= 5) %>%
  group_by(typical) %>%
  summarise(weighted_sum = sum(n), sum = n()) %>%
  pivot_longer(cols = c(sum, weighted_sum), names_to = "measure", values_to = "sum") %>%
  group_by(measure) %>%
  mutate(prop = sum / sum(sum)) %>%
  filter(typical)


typical_type_lmer <- glmer(typical ~ log(age) + (1|noun), 
                      data = parent_typical_ratings, 
     family = "binomial") %>% 
  tidy() %>%
  filter(effect == "fixed")

typical_token_lmer <- glmer(typical ~ log(age) + (1|noun), 
                      data = parent_typical_ratings, 
                      weights = n,
     family = "binomial") %>% 
  tidy() %>%
  filter(effect == "fixed")
```
```{r typical-effects}
typical_effect <- typical_token_lmer %>% filter(term == "log(age)") %>%
  pull(estimate)
typical_statistic <- typical_token_lmer %>% filter(term == "log(age)") %>% 
  pull(statistic)
typical_p <- typical_token_lmer %>% filter(term == "log(age)") %>% 
  pull(p.value) %>% printp()
```


While description at every age tended to point out atypical features, this effect changed in strength over development. An age effect added to the previous model was reliably negative, indicating that parents of older children are relatively more likely to focus on atypical features ($\beta =$ `r token_development_estimate`, $t =$ `r token_development_statistic`, $p$ = `r token_development_p`). In line with the idea that caregivers adapt their speech to their children's knowledge, it seems that caregivers are more likely to provide description of typical features for their young children, compared with older children. As a second test of this idea, we defined adjectives as highly typical if Turkers judged them to be 'often', 'almost always', or 'always' true. We predicted whether each judgment was highly typical from a mixed-effects logistic regression with a fixed effect of age (log-scaled) and a random effect of noun. Age was a highly reliable predictor ($\beta =$ `r typical_effect`, 
$t =$ `r typical_statistic`, $p$ `r typical_p`). While children at all ages hear more talk about what is atypically true (Figure  \ref{fig:distribution-plot}), younger children hear relatively more talk about what is typically true than older children do (Figure \ref{fig:prototypical-plot}).

```{r prototypical-plot, fig.align = "center", fig.width = 6, fig.height = 5, fig.cap = "Proportion of caregiver description that is about highly typical features (often, almost always, or always true), as a function of age. Rightmost point: the proportion of description in adult-adult speech that is about highly typical features."}

parent <- typicals_parent %>% 
  filter(measure == "weighted_sum") %>% 
  ggplot(aes(x = age,y = prop, colour = age)) +
  geom_smooth(method = "glm", formula = y~x,
                      method.args = list(family = gaussian(link = 'log')),color = "black") +
  geom_point(aes(fill=age), colour="black",pch=21, size=5) +
  ylab("Proportion of modifiers rated as \n highly typical of modified noun") +
  xlab("Child's Age (months)") +
  ylim(0.02,0.125) +
  facetted_pos_scales() +
  scale_x_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4))+
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme(#axis.line = element_line(colour = "black"),
        #axis.ticks = element_line(),
        #axis.text = element_text(size=14),
        #panel.grid = element_line(color="lightgrey",size=0.5),
        #axis.text.x = element_text(size=10, angle=15),
        legend.position = "none",
        aspect.ratio = 1/1.62) +
  theme(
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)

adult <- typicals_cabnc %>%
  mutate(age = 0) %>%
  filter(measure == "weighted_sum") %>% 
  ggplot(aes(x = age,y = prop, colour = age)) +
  geom_smooth(method = "glm", formula = y~x,
                      method.args = list(family = gaussian(link = 'log')),color = "black") +
  geom_point(aes(fill=age), colour="black",pch=21, size=5) +
  ylab("") +
  xlab("") +
  facetted_pos_scales() +
  ylim(0.02,0.125) +
  xlim(0,1)+
  scale_x_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4))+
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme(#axis.line = element_line(colour = "black"),
        #axis.ticks = element_line(),
        #axis.text = element_text(size=14),
        #panel.grid = element_line(color="lightgrey",size=0.5),
        #axis.text.x = element_text(size=10, angle=15),
        legend.position = "none",
        aspect.ratio = 1/1.62) +
  theme(
    axis.line.x=element_blank(),
    axis.text.x = element_blank(),
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    
)


old_plot <- typicals_parent %>% 
  mutate(corpus = "Parent speech") %>%
  rbind(typicals_cabnc %>% mutate(corpus = "Adult-Adult speech", age = 60)) %>%
  mutate(corpus = factor(corpus, levels = c("Parent speech", "Adult-Adult speech"))) %>%
  filter(measure == "weighted_sum") %>% 
  ggplot(aes(x = age,y = prop, colour = age)) +
  facet_grid2(cols = vars(corpus), scales = "free_x", space = "free_x") +
  geom_smooth(method = "glm", formula = y~x,
                      method.args = list(family = gaussian(link = 'log')),color = "black") +
  geom_point(aes(fill=age), colour="black",pch=21, size=5) +
  ylab("Proportion of modifiers rated as \n highly typical of modified noun") +
  xlab("Child's Age (months)") +
  facetted_pos_scales(
    x = list(corpus == "Parent speech" ~ scale_x_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4)),
             corpus == "Adult-Adult speech" ~ scale_x_continuous(minor_breaks = seq(58, 62, 2), breaks = seq(58, 62, 2)))
  ) +
  ylim(0.02,0.125) +
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme(#axis.line = element_line(colour = "black"),
        #axis.ticks = element_line(),
        #axis.text = element_text(size=14),
        #panel.grid = element_line(color="lightgrey",size=0.5),
        #axis.text.x = element_text(size=10, angle=15),
        legend.position = "none") +
  theme(
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)


typicals_plot <- typicals_parent %>% 
  mutate(corpus = "Child age in months (Parent speech)") %>%
  rbind(typicals_cabnc %>% mutate(corpus = "Adult-Adult", age = 70)) %>%
  mutate(corpus = factor(corpus, levels = c("Child age in months (Parent speech)", "Adult-Adult"))) %>%
  filter(measure == "weighted_sum") %>% 
  ggplot(aes(x = age,y = prop, colour = age)) +
  facet_wrap(~corpus, scales = "free_x", strip.position = "bottom") +
  geom_smooth(method = "glm", formula = y~x,
                      method.args = list(family = gaussian(link = 'log')),color = "black") +
  geom_point(aes(fill=age), colour="black",pch=21, size=5) +
  ylab("Proportion of modifiers rated as \n highly typical of modified noun") +
  xlab(NULL) +
  facetted_pos_scales(
    x = list(corpus == "Child age in months (Parent speech)" ~ 
               scale_x_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4)),
             corpus == "Adult-Adult" ~ 
               scale_x_continuous(labels = NULL, minor_breaks = NULL, breaks = seq(60,80,5)))) +
  ylim(0.02,0.125) +
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme(#axis.line = element_line(colour = "black"),
        #axis.ticks = element_line(),
        #axis.text = element_text(size=14),
        #panel.grid = element_line(color="lightgrey",size=0.5),
        #axis.text.x = element_text(size=10, angle=15),
        legend.position = "none") +
  theme(
    strip.background =element_blank(),
    panel.border = element_blank(),
    strip.placement = "outside",
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)

# convert ggplot object to grob object
gp <- ggplotGrob(typicals_plot)

# optional: take a look at the grob object's layout
#gtable::gtable_show_layout(gp)

# get gtable columns corresponding to the facets (5 & 9, in this case)
facet.columns <- gp$layout$l[grepl("panel", gp$layout$name)]

# get the number of unique x-axis values per facet (1 & 3, in this case)
x.var <- c(12,2)

# change the relative widths of the facet columns based on
# how many unique x-axis values are in each facet
gp$widths[facet.columns] <- gp$widths[facet.columns] * x.var

# plot result
grid::grid.draw(gp)
```

```{r read-data-all-utts}
# full corpus with utterance ids
  # note actual utterances are redacted for data sharing 
  # to preserve anonymity while sharing as much as possible
ldp <- read_csv(here("../data/ldp_full_corpus_redacted.csv"))

ldp_adjs <- ldp %>%
  left_join(ldp_coded, by = c("id" = "doc_id", "subject", "session", "person")) %>%
  arrange(id)

ldp_lag_pairs <- ldp_adjs %>%
  select(utt_id, subject, session, person, adj_token, noun_token) %>%
  mutate(adj_noun_phrase = paste(adj_token, noun_token)) %>%
  paste.data.frame(term = "adj_noun_phrase", 
                   group = c("utt_id", "subject", "session", "person"), 
                   collapse = " ") %>%
  mutate(adj_noun_phrase = trimws(str_remove_all(adj_noun_phrase, "NA")),
         adj_noun_phrase = str_replace_all(adj_noun_phrase, "  ", " "),
         adj_noun_phrase = str_replace_all(adj_noun_phrase, "  ", " ")) %>%
  mutate(parent_adj_noun_phrase = if_else(person == "parent",
                                          adj_noun_phrase, "")) %>%
  group_by(subject, session) %>%
  mutate(lag_data = paste(lag(parent_adj_noun_phrase), 
                          lag(parent_adj_noun_phrase, 2), lag(parent_adj_noun_phrase, 3),
                          lag(parent_adj_noun_phrase, 4), lag(parent_adj_noun_phrase, 5))) %>%
  ungroup() %>%
  mutate(lag_data = trimws(str_remove_all(lag_data, "NA")),
         lag_data = str_replace_all(lag_data, "  ", " "),
         lag_data = str_replace_all(lag_data, "  ", " ")) 
   
ldp_adjs_rep <- ldp_adjs %>%
  left_join(ldp_lag_pairs %>% select(-adj_noun_phrase), by = c("utt_id", "person", "subject", "session"))
```

```{r child-ridge-plot, fig.align = "center", fig.width=3.4, fig.height = 5.4, fig.cap = "Density plots showing children's use of atypical and typical adjective-noun pairs across age.", fig.pos="tb", eval = FALSE}
ldp_coded %>%
  filter(person == "child") %>%
  mutate(typicality=mean_rating) %>%
  group_by(session) %>%
  mutate(age = min(age)) %>%
  ggplot(aes(x = typicality, y=age, group=age, fill=age)) +
  geom_density_ridges2() +
  ylab("Child Age (months)") +
  xlab("More Atypical                   More Typical \n Typicality of adjective-noun pairs") +
  geom_vline(xintercept = 4, size=1, linetype="solid")+
  scale_fill_gradient(low="cornsilk", high=muted("green")) +
  theme_few() +
  theme(#panel.grid = element_line(color="lightgrey",size=0.5), 
    axis.line = element_line(colour = "black"),
    axis.ticks = element_line(),
    axis.text.x = element_text(size=11, angle=28, hjust=1),
    axis.text.y = element_text(size=11),
    legend.position = "none") +
  scale_x_continuous(minor_breaks = seq(1 , 7, 1), breaks = seq(1, 7, 1), labels = c('never', 'rarely', 'sometimes', 'about half', 'often', 'almost always', 'always')) +
  scale_y_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4)) +
  theme(
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)

```
```{r child-non-reps-plot, fig.width=3.4, fig.height = 5.4, fig.cap = "Density plots showing children's use of atypical and typical adjective-noun pairs across age after excluding repeated utterances.", fig.pos="tb"}
ldp_child_non_rep <- ldp_adjs_rep %>%
  filter(person == "child", !is.na(adj_token), !is.na(noun_token)) %>%
  mutate(adj_noun_phrase = paste(adj_token, noun_token)) %>%
  filter(!str_detect(lag_data, adj_noun_phrase)) %>%
  left_join(mean_ratings, by = c("adj_token" = "adjective", "noun_token" = "noun")) %>%
  filter(!is.na(mean_typ)) %>% # filter out nonsense pairs
  mutate(age = (4*session + 10))

ldp_child_non_rep %>%
  mutate(typicality=mean_typ) %>%
  group_by(session) %>%
  mutate(age = min(age)) %>%
  ggplot(aes(x = typicality, y=age, group=age, fill=age)) +
  geom_density_ridges2() +
  ylab("Child Age (months)") +
  xlab("More Atypical                   More Typical \n Typicality of adjective-noun pairs") +
  geom_vline(xintercept = 4, size=1, linetype="solid")+
  scale_fill_gradient(low="cornsilk", high=muted("green")) +
  theme_few() +
  theme(#panel.grid = element_line(color="lightgrey",size=0.5), 
    axis.line = element_line(colour = "black"),
    axis.ticks = element_line(),
    axis.text.x = element_text(size=11, angle=28, hjust=1),
    axis.text.y = element_text(size=11),
    legend.position = "none") +
  scale_x_continuous(minor_breaks = seq(1 , 7, 1), breaks = seq(1, 7, 1), labels = c('never', 'rarely', 'sometimes', 'about half', 'often', 'almost always', 'always')) +
  scale_y_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4)) +
  theme(
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)

```

```{r kid-rep-model}
kid_tokens_no_rep <- ldp_child_non_rep %>%
  count(session, age, adj_token, noun_token, mean_typ) %>%
  filter(!is.na(mean_typ)) %>%
  mutate(centered_rating = mean_typ - 4)

kid_model_no_rep <- lmer(centered_rating ~ 1 + (1|noun_token), weights = n, 
                       data = kid_tokens_no_rep) %>%
                     tidy() %>%
                     filter(effect == "fixed")

kid_no_rep_token_estimate <- kid_model_no_rep %>% filter(term == "(Intercept)") %>%
  pull(estimate)
kid_no_rep_token_statistic <- kid_model_no_rep %>% filter(term == "(Intercept)") %>%
  pull(statistic)
kid_no_rep_token_p <- kid_model_no_rep %>% filter(term == "(Intercept)") %>%
  pull(p.value) %>% printp()

kid_model_no_rep_type <- lmer(centered_rating ~ 1 + (1|noun_token),  
                       data = kid_tokens_no_rep) %>%
                     tidy() %>%
                     filter(effect == "fixed")

kid_no_rep_type_estimate <- kid_model_no_rep %>% filter(term == "(Intercept)") %>%
  pull(estimate)
kid_no_rep_type_statistic <- kid_model_no_rep %>% filter(term == "(Intercept)") %>%
  pull(statistic)
kid_no_rep_type_p <- kid_model_no_rep %>% filter(term == "(Intercept)") %>%
  pull(p.value) %>% printp()

kid_model_no_rep_token_byage <- kid_tokens_no_rep %>%
  filter(age > 18) %>%
  group_by(age) %>%
  nest() %>%
  mutate(effect = map(data, ~lmer(centered_rating ~ 1 + (1|noun_token),
                                  weights = n, data = .) %>%
                       tidy() %>%
                       filter(effect == "fixed"))) %>%
  select(-data) %>%
  unnest(cols = c(effect)) %>%
  ungroup()

kid_youngest_estimate <- kid_model_no_rep_token_byage %>% filter(age == 22, term == "(Intercept)") %>%
  pull(estimate)
kid_youngest_statistic <- kid_model_no_rep_token_byage %>% filter(age == 22, term == "(Intercept)") %>%
  pull(statistic)
kid_youngest_p <- kid_model_no_rep_token_byage %>% filter(age == 22, term == "(Intercept)") %>%
  pull(p.value) %>% printp()
```

### Child Speech.

Given the striking consistency in adult-to-adult speech and caregiver speech across ages, we next consider what kind of information is contained in children's speech. By analyzing children's own utterances, we can determine when children come to use description in a way that looks like adult speech. Are children mirroring adult-like uses of description even from a young age, or are they choosing to describe more typical features of the world?

We analyzed children's use of adjective--noun pairs and found that, following the pattern of parent speech and adult-adult speech, they predominantly mention atypical rather than typical features; confirmed statistically as above, we find a reliably negative intercept ($\beta =$ `r kid_token_estimate`, $t =$ `r kid_token_statistic`, $p$ `r kid_token_p`). One deflationary explanation for this pattern is that children are simply often repeating the adjective-noun pairs their parents just produced. To rule out this explanation, we re-analyzed the data excluding any adjective-noun pairs produced by a parent in the past five utterances in conversation, still finding a reliably negative intercept ($\beta =$ `r kid_no_rep_token_estimate`, $t =$ `r kid_no_rep_token_statistic`, $p$ `r kid_no_rep_token_p`). Further, when testing within each age group, even the 22-month-olds (the first age for which we have sufficient child adjective-noun utterances to estimate) are reliably producing more atypical than typical adjective-noun pairs; the intercept is reliably negative when estimated within every age (14-month-olds and 18-month-olds are excluded due to having 0 and 3 adjective-noun pairs, respectively; estimate at 22 months old, $\beta =$ `r kid_youngest_estimate`, $t =$ `r kid_youngest_statistic`, $p$ `r kid_youngest_p`) That is, even when excluding utterances children may have immediately imitated from their parents, and from the earliest ages they are consistently using adjective-noun pairs, children more often mention atypical than typical features of things (Figure \ref{fig:child-non-reps-plot}).

The fact that children are remarking on atypical features is intriguing, but it would be premature to conclude that they are doing so to be selectively informative. Note also that especially at young ages, children produce few adjective-noun pairs—they are not producing any at 14 months old, our earliest timepoint—so our data on children's speech is somewhat sparse. We discuss potential interpretations of this finding further in the Conclusion.

<!-- Mirroring caregiver and adult-adult speech, children's productions show a positive skew (0.61, comapre with skewness = 0.65 seen in the adults), such that the bulk of language reflects adjective-noun pairs rated < 4 on typicality (i.e. 'sometimes', 'rarely' or 'never'). -->

## Discussion

In sum, we find robust evidence that language is used to discuss atypical, rather than typical, features of the world. Description in caregiver speech seems to largely mirror the usage patterns that we observed in adult-to-adult speech, suggesting that these patterns arise from general communicative pressures. Interestingly, the descriptions children hear change over development, becoming increasingly focused on atypical features. The higher prevalence of typical descriptors in early development may help young learners learn what is typical; however, even at the earliest point we measured, the bulk of language input describes atypical features. 

Considering evidence of an atypicality bias in children's own utterances, it should be noted that children's utterances come from naturalistic conversations with caregivers, and their use of atypical description may be prompted by parent-led discourse. That is, if a caregiver chooses to describe the *purpleness* of a cat in book, the child may well respond by asking about that same feature. Further, atypical descriptors may actually be more likely to elicit imitation  from child speakers, compared with typical descriptors [@bannard2017]. While our analyses rule out the role of immediate imitation, future work is needed to better disentangle the extent to which children's productions reflect caregiver-led discourse. 

<!-- their pattern of description is likely highly dependent on their caregiver's utterances.... Thus, it is possible that some of the children's use of atypical description is prompted by a parent-led discourse. -->

<!-- Interestingly, the descriptions children hear change over development, becoming increasingly focused on atypical features. The higher prevalence of typical descriptors in early development may help young learners learn what is typical; however, even at the earliest point we measured, the bulk of language input describes atypical features. -->

<!-- Across adult, parent, and child language corpora, we find robust evidence that language use systematically overerpresents atypical features.  -->

This usage pattern aligns with the idea that language is used informatively in relation to background knowledge about the world. It may pose a problem, however, for young language learners with still-developing world knowledge. If language does not transparently convey the typical features of objects, and instead (perhaps misleadingly) notes the atypical ones, how might children come to learn what objects are typically like? One possibility is that information about typical features is captured in more complex regularities across many utterances. If this is true, language may still be an important source of information about typicality as children may be able to extract more accurate typicality information by tracking statistical regularities across many utterances.

# Extracting Typicality from Language Structure

We have shown that language — between adults, from adults to children, and from children themselves — is robustly used to comment on the atypical features of things.  On the surface, this makes should make it difficult for language alone to capture information about what is typical.  However, thus far we have focused on information found in directly co-occurring adjective noun pairs, and it is possible that accurate typicality information can be found beneath the surface in the deeper structure of language (e.g., second-order co-occurrences). 

Indeed, much information can be gleaned from language that does not seem available at first glance. From language alone, simple distributional learning models can recover enough information to perform comparably to non-native college applicants on the Test of English as a Foreign Language [@landauer1997]. Recently, @lewis2019 demonstrated that even nuanced feature information may be learnable through distributional semantics alone, without any complex inferential machinery. Further, experiments with adults and children suggest that co-occurrence regularities may help structure semantic knowledge [@unger_statistical_2020; @savic_experience_2023; @savic_exposure_2022]. Relationships among nouns that reflect feature information such as size are recoverable in the semantic spaces of large language models [@grand2022semantic]. However, language models show deficits in inferring typicality and atypicality in more controlled tasks, departing systematically from human-like pragmatic inference [@kurch2024large; @misra2021language].

Here, we ask whether a simple distributional semantics model trained on the language children hear can capture typical feature information. Further, we test whether a distributional semantics model trained on a larger corpus of adult-directed text as well as two more sophisticated language models capture adjective-noun typicality. These models are trained on more and different language than is available to children, but tell us more about whether and how typicality information is learnable by applying simple learning objectives to text.  

```{r word2vec-cors}
ldp_cor <- wtd.cor(all_judgments %>% pull(mean_typ),
                    all_judgments %>% pull(ldp_similarity)) %>%
  as_tibble()

wiki_cor <- wtd.cor(all_judgments %>% pull(mean_typ),
                   all_judgments %>% pull(wiki_similarity)) %>%
  as_tibble()

bert_cor <- wtd.cor(all_judgments %>% pull(mean_typ),
                    all_judgments %>% pull(bert_prob)) %>%
  as_tibble()

gpt3_cor <- wtd.cor(all_judgments %>% pull(mean_typ),
                    all_judgments %>% pull(gpt3_score)) %>%
  as_tibble()

models_cor <- wtd.cor(all_judgments %>% pull(ldp_similarity),
                    all_judgments %>% pull(wiki_similarity)) %>%
  as_tibble()

bert_wikiw2v_cor <- wtd.cor(all_judgments %>% pull(bert_prob),
                    all_judgments %>% pull(wiki_similarity)) %>%
  as_tibble()

ldp_cor_estimate <- ldp_cor %>% pull(correlation)
ldp_cor_p <- ldp_cor %>% pull(p.value) %>% printp()


wiki_cor_estimate <- wiki_cor %>% pull(correlation)
wiki_cor_p <- wiki_cor %>% pull(p.value) %>% printp()

bert_cor_estimate <- bert_cor %>% pull(correlation)
bert_cor_p <- bert_cor %>% pull(p.value) %>% printp()

gpt3_cor_estimate <- gpt3_cor %>% pull(correlation)
gpt3_cor_p <- gpt3_cor %>% pull(p.value) %>% printp()

w2v_models_cor_estimate <- models_cor %>% pull(correlation)
w2v_models_cor_p <- models_cor %>% pull(p.value) %>% printp()
w2v_bert_cor_estimate <- bert_wikiw2v_cor %>% pull(correlation)
w2v_bert_cor_p <- bert_wikiw2v_cor %>% pull(p.value) %>% printp()

```

```{r, eval = FALSE}
split_half <- tidy_turk_counts %>%
  mutate(rater = as.numeric(gsub("x", "", rater))) %>%
  filter(rater %in% c(1, 2)) %>%
  group_by(rater, noun, adj) %>%
  summarise(score = mean(score, na.rm = T)) %>%
  pivot_wider(names_from = rater, values_from = score)

half_model <- lmer(`1` ~ `2` + (1|adj) + (1|noun), data = split_half)

split_half %>%
  ungroup() %>%
  mutate(prediction = predict(half_model)) %>%
  summarise(cor = cor(`1`, prediction))

data <- tidy_turk_counts_for_plots %>% 
       left_join(tidy_turk_counts %>% filter(rater == "x1"))

cor(data$score, data$wiki_similarity, use = "pairwise")

wiki_model <- lm(score ~ wiki_similarity ,
     data = data) 

tidy_turk_counts_for_plots %>%
  filter(!is.na(wiki_similarity)) %>%
  mutate(prediction = predict(wiki_model)) %>%
  summarise(cor = cor(prediction, turker_judgment))

```

```{r word2vec-pairs}
min_max_ratings <- mean_ratings %>%
  group_by(noun) %>%
  mutate(min_typ = min(mean_typ), max_typ = max(mean_typ)) %>%
  distinct(noun, min_typ, max_typ) %>%
  filter(min_typ != max_typ, max_typ >= 5, min_typ <= 3) 


high_low_pairs <- mean_ratings %>%
  filter(noun %in% min_max_ratings$noun) %>%
  left_join(min_max_ratings, by = c("noun")) %>%
  filter(mean_typ == min_typ | mean_typ == max_typ) %>%
  select(adjective, noun, mean_typ) %>%
  group_by(noun) %>%
  arrange(noun, desc(mean_typ)) %>%
  slice(1, n()) %>%
  mutate(typicality = c("high", "low"),
         typicality = factor(typicality, levels = c("low", "high"))) %>%
  left_join(all_corpus_counts, by = c("adjective", "noun"))

high_low_all <- high_low_pairs %>%
  left_join(all_judgments %>% select(-is_multi_token)) %>%
  mutate(typicality = c("high", "low"),
         typicality = factor(typicality, levels = c("low", "high")))


correct_orders <- high_low_all %>%
  select(-n) %>%
  pivot_longer(cols = c(ldp_similarity, wiki_similarity, bert_prob, gpt3_score), 
               names_to = "measure", values_to = "similarity") %>%
  select(-adjective, -mean_typ, -gpt3_judgment) %>%
  pivot_wider(names_from = "typicality", values_from = "similarity") %>%
  mutate(correct = high - low > 0) %>%
  filter(!is.na(correct)) %>%
  group_by(measure) %>%
  summarise(correct = sum(correct), total = n())

ldp_correct <- correct_orders %>%
  filter(measure == "ldp_similarity") %>%
  pull(correct)

wiki_correct <- correct_orders %>%
  filter(measure == "wiki_similarity") %>%
  pull(correct)

bert_correct <- correct_orders %>%
  filter(measure == "bert_prob") %>%
  pull(correct)

gpt3_correct <- correct_orders %>%
  filter(measure == "gpt3_score") %>%
  pull(correct)

pairs_total <- correct_orders %>% pull(total) %>% first()

ldp_binom <- binom.test(ldp_correct, pairs_total)$p.value %>%
  printp()
wiki_binom <- binom.test(wiki_correct, pairs_total)$p.value %>%
  printp()
bert_binom <- binom.test(bert_correct, pairs_total)$p.value %>%
  printp()
gpt3_binom <- binom.test(gpt3_correct, pairs_total)$p.value %>%
  printp()
```
```{r typ-plots-compute}

# The black dotted line shows average human typicality ratings (scaled) for these items. The blue line shows how well our models do at capturing this trend, with grey lines representing individual pairs

wiki_min = min(all_judgments$wiki_similarity, na.rm = TRUE)
wiki_max = max(all_judgments$wiki_similarity, na.rm = TRUE)
ldp_min = min(all_judgments$ldp_similarity, na.rm = TRUE)
ldp_max = max(all_judgments$ldp_similarity, na.rm = TRUE)
bert_min = min(all_judgments$bert_prob, na.rm = TRUE)
bert_max = max(all_judgments$bert_prob, na.rm = TRUE)

halves_data <- high_low_all %>%
  select(-gpt3_judgment) %>%
  pivot_longer(cols = c(mean_typ, wiki_similarity, ldp_similarity, bert_prob, gpt3_score),
              names_to = "measure", values_to = "score") %>%
  mutate(measure = factor(measure, 
                          levels = c("mean_typ", "ldp_similarity", 
                                     "wiki_similarity", "bert_prob", "gpt3_score"), 
                          labels = c("Human", "LDP word2vec", 
                                     "Wiki word2vec", "BERT", "GPT-3"))) %>%
  mutate(score = if_else(measure=="Human", (score - 1)/6, score)) 

human <- halves_data %>%
  filter(measure =="Human") %>%
  group_by(typicality) %>% 
  summarise(true_mean=mean(score)) %>%
  mutate(wiki_similarity = true_mean*(wiki_max - wiki_min) + wiki_min,
         ldp_similarity = true_mean*(ldp_max - ldp_min) + ldp_min,
         bert_prob = true_mean*(bert_max - bert_min) + bert_min,
         gpt3_score = true_mean*1) %>%
  pivot_longer(c(true_mean, wiki_similarity, ldp_similarity, bert_prob, gpt3_score),
               names_to = "measure", values_to = "mean") %>%
  mutate(measure = factor(measure, 
                          levels = c("true_mean", "ldp_similarity", 
                                     "wiki_similarity", "bert_prob", "gpt3_score"), 
                          labels = c("True mean", "LDP word2vec", 
                                     "Wiki word2vec", "BERT", "GPT-3"))) 

means <- halves_data %>%
    group_by(measure, typicality) %>% 
    summarise(mean=mean(score, na.rm=T)) %>%
  mutate(mean = if_else(measure=="GPT-3", (mean - 1)/6, mean)) %>%
    filter(measure != "Human") 

grob <- grobTree(textGrob("Human ratings \n (scaled)", x=0.97,  y=0.85, hjust=1,
  gp=gpar(col="Black", fontsize=9, fontface="italic")))
  
grob2 <- grobTree(textGrob("Model average", x=.96,  y=0.43, hjust=1,
  gp=gpar(col="steelblue", fontsize=9, fontface="italic")))

typicality_axis <- c("Low Typicality", "High Typicality")

```


## Method

To test the possibility that simple distributional semantics models would capture typicality relationships between nouns and adjectives, we trained word2vec on the same corpus of child-directed speech used in our first set of analyses. Word2vec is a neural network model that learns to predict words from the contexts in which they appear. This leads word2vec to encode words that appear in similar contexts as similar to one another [@firth1957].

We used the continuous-bag-of-words (CBOW) implementation of word2vec in the `gensim` package [@rehurek2010]. We trained the model using a surrounding context of 5 words on either side of the target word and 100 dimensions (weights in the hidden layer) to represent each word. After training, we extracted the hidden layer representation of each word in the model's vocabulary---these are the vectors used to represent these words. 

If the model captures information about the typical features of objects, we should see that the model's noun-adjective word pair similarities are correlated with the typicality ratings we elicited from human raters. For a second comparison, we also used an off-the-shelf implementation of word2vec trained on Wikipedia [@mikolov2018]. While the Language Development Project corpus likely underestimates the amount of structure in children's linguistic input, Wikipedia likely overestimates it.

While word2vec straightforwardly represents what can be learned about word similarity by associating words with similar contexts, it does not represent the cutting edge of language modeling. Perhaps more sophisticated models trained on larger corpora would represent these typicalities better. To test this, we asked how BERT [@devlin2018] and GPT-3 [@brown_language_2020] represent typicality. BERT is a masked language model trained on BookCorpus and English Wikipedia, which represents the probability of words occurring in slots in a phrase. We gave BERT phrases of the form "____ apple", and asked it the probability of different adjectives filling the empty slot. 

GPT-3 is a generative language model trained on large quantities of internet text, including Wikipedia, book corpora, and web page text from crawling the internet. Because it is a generative language model, we can ask GPT-3 the same question we asked human participants directly and it can generate a text response. We prompted the `davinci-text-003` instance of GPT-3 questions of the form: "You are doing a task in which you rate how common it is for certain things to have certain features. You respond out of the following options: Never, Rarely, Sometimes, About half the time, Often, Almost always, or Always. How common is it for a cow to be a brown cow?" Because BERT and GPT-3 are trained on more and different kinds of language than what children hear, results from these models likely do not straightforwardly represent the information available to children in language. However, results from BERT and GPT-3 can indicate the challenges language models face in representing world knowledge when the language people use emphasizes remarkable rather than typical features.

## Results

We find that similarities in the model trained on the Language Development Project corpus have near zero correlation with human adjective–noun typicality ratings ($r =$ `r ldp_cor_estimate`, $p =$ `r ldp_cor_p`). However, our model does capture other meaningful information about the structure of language, such as similarity within part of speech categories. Comparing with pre-existing large-scale human similarity judgements for word pairs, our model shows significant correlations (correlation with wordsim353 similarities of noun pairs, 0.28; correlation with simlex similarities of noun, adjective, and verb pairs, 0.16). This suggests that statistical patterns in child-directed speech are likely insufficient to encode information about the typical features of objects, despite encoding at least some information about word meaning more broadly. 

However, the corpus on which we trained this model was small; perhaps our model did not get enough language to draw out the patterns that would reflect the typical features of objects. To test this possibility, we asked whether word vectors trained on a much larger corpus—English Wikipedia—correlate with typicality ratings. This model's similarities were significantly correlated with human judgments, although the strength of the correlation was still fairly weak ($r =$ `r wiki_cor_estimate`, $p$ `r wiki_cor_p`). How do larger and more sophisticated language models fare? Like Wikipedia-trained word2vec, BERT's probabilities were significantly correlated with human judgments, though weakly so ($r =$ `r bert_cor_estimate`, $p$ `r bert_cor_p`). However, GPT-3's ratings were much better aligned with human judgments ($r =$ `r gpt3_cor_estimate`, $p$ `r gpt3_cor_p`). 

Similarity judgments produced by our models reflect many dimensions of similarity, but our human judgments reflect only typicality. To account for this fact and control for semantic differences among the nouns in our set, we performed a second analysis in which we considered only the subset of `r pairs_total` nouns that had both a high-typicality (rated as at least "often") and a low-typicality (rated as at most "sometimes") adjective. We then asked whether the word2vec models rated the high-typicality adjective as more similar to the noun it modified than the low-typicality adjective. The LDP model correctly classified `r ldp_correct` out of `r pairs_total` (`r (ldp_correct / pairs_total)*100`%), which was not different from chance ($p =$ `r ldp_binom`). The Wikipedia-trained word2vec model correctly classified `r wiki_correct` out of `r pairs_total` (`r (wiki_correct / pairs_total)*100`%), which was better than chance according to a binomial test, though not highly accurate ($p$ `r wiki_binom`). Figure \ref{fig:halfs} shows the word2vec models' similarities for the `r pairs_total` nouns and their typical and atypical adjectives alongside scaled average human ratings. 

The analogous analysis on BERT asks whether the model rates the high-typicality adjective as more likely to come before the noun than the low typicality adjective (e.g., P("red") > P("brown") in "____ apple"). BERT correctly classified `r bert_correct` out of `r pairs_total` (`r (bert_correct / pairs_total)*100`%), which is significantly better than chance ($p =$ `r bert_binom`). However, BERT's performance was directionally less accurate than Wikipedia-trained word2vec: though BERT is a more sophisticated model, it does not capture adjective-noun typicality better than word2vec in this analysis. GPT-3 performs much better than BERT and the word2vec models, with `r gpt3_correct` out of `r pairs_total` (`r (gpt3_correct / pairs_total)*100`%; $p$ `r gpt3_binom`). Figure \ref{fig:bert-gpt} shows BERT and GPT-3 ratings for the `r pairs_total` nouns and their typical and atypical adjectives alongside scaled average human ratings. 

```{r halfs, cache=F, fig.align = "center", fig.width = 6, fig.cap = 'Plots of word2vec noun-adjective similarities for nouns for which there was at least one atypical adjective (rated at most "sometimes"), and at least one typical adjective (rated at least "often"). Human ratings line depicts the mean human rating in each group, scaled to the range of model outputs.'}

halves_data %>%
  filter(measure %in% c("Wiki word2vec", "LDP word2vec")) %>%
  mutate(measure = factor(measure, 
                          levels = c("LDP word2vec", "Wiki word2vec"))) %>%
  ggplot(aes(x = typicality, y = score, group = noun)) +
  facet_wrap(~measure) +
  geom_point(alpha = .5, position=position_dodge(0.06), color = "light gray") +
  geom_line(alpha = .5, color = "light gray", position=position_dodge(0.06)) +
  geom_line(data=human %>% filter(measure %in% c("LDP word2vec", "Wiki word2vec")),
            aes(x=typicality, y=mean, group=NA), color="black", linetype="dashed") +
  geom_point(data=human %>% filter(measure %in% c("LDP word2vec", "Wiki word2vec")),
             aes(x=typicality, y=mean, group=NA), color="black") +
  geom_line(data=means %>% filter(measure %in% c("LDP word2vec", "Wiki word2vec")),
            aes(x=typicality, y=mean, group=NA), color="steelblue", size=.8) +
  geom_point(data=means %>% filter(measure %in% c("LDP word2vec", "Wiki word2vec")), 
             aes(x=typicality, y=mean, group=NA), color="steelblue") +
  labs(x = "Binned Human-Rated Typicality", y = "Cosine Similarity") +
  theme_few() +
  coord_cartesian(ylim=c(-0.17,1)) +
  annotation_custom(grob) +
  annotation_custom(grob2) +
  scale_x_discrete(labels= typicality_axis, expand = c(.2, .2)) +
  theme(
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)

#ggsave(bertplot, filename = "figs/bert_w2v_plot.png",  width = 8, height = 4, units = "in", dpi = 300, bg = "transparent")
```

```{r bert-gpt, cache=F, fig.align = "center", fig.width = 6, fig.cap = 'Plots of BERT and GPT-3 noun-adjective similarities for nouns for which there was at least one atypical adjective (rated at most "sometimes"), and at least one typical adjective (rated at least "often"). Human ratings line depicts the mean human rating in each group, scaled to the range of model outputs.'}

halves_data %>%
  filter(measure %in% c("BERT", "GPT-3")) %>%
  mutate(score = if_else(measure=="GPT-3", (score - 1)/6, score)) %>%
  #mutate(score = if_else(measure=="BERT", log(score), score)) %>%
  mutate(measure = factor(measure, 
                          levels = c("BERT", "GPT-3"))) %>%
  ggplot(aes(x = typicality, y = score, group = noun)) +
  facet_wrap(~measure) +
  geom_point(alpha = .5, position=position_dodge(0.06), color = "light gray") +
  geom_line(alpha = .5, color = "light gray", position=position_dodge(0.06)) +
  geom_line(data=human %>% filter(measure %in% c("BERT", "GPT-3")),
            aes(x=typicality, y=mean, group=NA), color="black", linetype="dashed") +
  geom_point(data=human %>% filter(measure %in% c("BERT", "GPT-3")),
             aes(x=typicality, y=mean, group=NA), color="black") +
  geom_line(data=means %>% filter(measure %in% c("BERT", "GPT-3")),
            aes(x=typicality, y=mean, group=NA), color="steelblue", size=.8) +
  geom_point(data=means %>% filter(measure %in% c("BERT", "GPT-3")), 
             aes(x=typicality, y=mean, group=NA), color="steelblue") +
  labs(x = "Binned Human-Rated Typicality", y = "Probability for BERT \n Scaled rating for GPT-3") +
  theme_few() +
  coord_cartesian(ylim=c(-0.17,1)) +
  annotation_custom(grob) +
  annotation_custom(grob2) +
  scale_x_discrete(labels= typicality_axis, expand = c(.2, .2)) +
  theme(
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)

#ggsave(bertplot, filename = "figs/bert_w2v_plot.png",  width = 8, height = 4, units = "in", dpi = 300, bg = "transparent")
```

```{r pairs_tab, eval = FALSE}
pair_tab <- high_low_pairs %>%
  group_by(noun) %>%
  summarise(diff = first(wiki_similarity) - last(wiki_similarity)) %>%
  filter(diff < 0) %>%
  left_join(high_low_pairs %>% select(noun, adj)) %>%
  group_by(noun) %>%
  mutate(typicality = c("high", "low")) %>%
  pivot_wider(names_from = "typicality", values_from = "adj") %>%
  select(diff, noun, high, low) %>%
  arrange(diff) %>%
  ungroup() %>%
  # slice(1:10) %>%
  slice(1:6) %>%
  select(-diff) %>%
  rename("typical adjective" = "high",
         "atypical adjective" = "low") %>%
  xtable(caption = "The top six cases in which Wikipedia-trained word2vec similarities were worst at predicting human typicality judgments. In each case, word2vec judged the low-typicality adjective to be more similar to the noun than the high-typicality adjective.",
         label = "tab:pairs_tab")

print(pair_tab, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
      include.rownames = FALSE)

```

# General Discussion

For models and the developing learner alike, language provides a rich source of information about the world. However, this information is not always transparently available: because language is used to comment on the atypical, it does not perfectly mirror the world. Among adult conversational partners whose world knowledge is well-aligned, this allows people to converse informatively and avoid redundancy. But between a child and caregiver whose world knowledge is asymmetric, this pressure competes with other demands: what is minimally informative to an adult may be misleading to a child. Our results show that this pressure structures language to create a peculiar learning environment, one in which caregivers predominantly point out the atypical features of things. 

How, then, do children learn about the typical features of things? While younger children may gain an important foothold from hearing more description of typical features, they still face language dominated by atypical description. When we looked at more nuanced ways of extracting information from language (which may or may not be available to the developing learner), we found that two word2vec models, one trained on child-directed language and one trained on adult-adult language, did not capture typicality very well. Even BERT, a language model trained on much more text and with a more complex architecture, did not perform better than a Wikipedia-trained word2vec model in reflecting typicality. This may be because these models are designed to capture language statistics, with BERT in particular capturing which words are likely to occur following one another—and as we show in our corpus analyses, adjective-noun pairs that come together often reflect atypicality rather than typicality. Note that a consistent *inverse* relationship—rating high-typicality pairs as *less* similar or *less* probable—would also be evidence that these models capture typicality, but the word2vec models and BERT do not evince this pattern either. However, GPT-3 captured typicality quite well, suggesting that the way people structure language to emphasize atypicality is not necessarily an impediment for much larger models' representation of typicality. Further work remains to understand how GPT-3 comes to represent typicality relationships so much better than the smaller models we tested. Overall, a large language model trained on text much greater in quantity and different in quality from child-directed language did capture adjective-noun typicality well, but models with simpler learning mechanisms and language input more similar to what is available to children did not.

Of course, perceptual information from the world may simplify the problem of learning about typicality. In many cases, perceptual information may swamp information from language; children likely see enough orange carrots in the world to outweigh hearing "purple carrot." It remains unclear, however, how children learn about categories for which they have scarcer evidence. Indeed, language information likely swamps perceptual information for many other categories, such as abstract concepts or those that cannot be learned about by direct experience. If such concepts pattern similarly to the concrete objects analyzed here, children are in a particularly difficult bind. 

It is also possible that other cues from language and interaction provide young learners with clues to what is typical or atypical, and these cues are uncaptured by our measure of usage statistics. Caregivers may highlight when a feature is typical by using certain syntactic constructions, such as generics (e.g., "tomatoes are red"), and children may learn especially well from rarer constructions that use adjectives postnominally or contrast among referents present in the discourse context [@davies2020adjective; @au1987acquiring; @waxman2000role]. Caregivers may also mark the atypicality of a feature using extralinguistic cues, e.g., by demonstrating surprise using prosody and facial expressions. Such cues from language and interaction may provide key cues to interpretation; however, given the sheer frequency of atypical descriptors, it seems unlikely that they are consistently well-marked.

<!-- Though perceptual information is undoubtedly useful in learning about the typical features of things, it remains to be explained how children learn what is typical when this perceptual information is scant, irrelevant, or incomplete. -->

<!-- Given that the present work is limited to concrete concepts, we can only speculate about the information caregivers provide about abstract concepts. But,  -->

Another possibility is that children expect language to be used informatively at a young age. Under this hypothesis, their language environment is not misleading at all, even without additional cues from caregivers. Children as young as two years old tend to use words to comment on what is new rather than what is known or assumed [@baker1988;@bohn2021young]. Children may therefore expect adjectives to comment on surprising features of objects. If young children expect adjectives to mark atypical features or events [@horowitz_childrens_2016], as adults do [@bergey2023using;@kravtchenko2022informationally], they can use description and the lack thereof to learn more about the world. Our finding that children themselves mostly remark on atypical rather than typical features of things is consistent with this possibility, though does not provide strong evidence that children understand to use description informatively.

<!-- It is further possible that children are tracking more nuanced syntactic information, which could provide helpful cues to what is typical (e.g., adjectives used in a generic) -->

<!--and may provide roughly informative speech themselves by age 5-6 [@nadig2002].  More naturalistic studies find that children at the one-word stage selectively imitate words that convey new information—that is, words that are informative with respect to what is obvious or assumed in the environment—when describing events (Greenfield & Zukow, 1978). -->

Whether adult-directed, child-directed, or a child's own speech, language is used with remarkable consistency: people talk about the atypical. Though parents might have reasonably been broadly over-informative in order to calibrate to their children's limited world knowledge, this is not the case. This presents a potential puzzle for young learners who have limited world knowledge and limited pragmatic inferential abilities. Indeed, only cutting-edge language models with extensive (and developmentally implausible) training data are able to solve this puzzle, leaving other sophisticated models stumped. For human learners, perceptual information and nascent pragmatic abilities may help fill in the gaps, but much remains to be explored to link these explanations to actual learning. The pressure for language to be informative is a pervasive force structuring language at every level, and future work must disentangle whether children capitalize on or are misled by this selective informativity in learning about the world.

# Data and Materials 

Stimuli, data, and analysis code available at\ \url{https://osf.io/ypdzv/?view_only=82d769d2963f4a30b24d50588c3d047d}.

# Acknowledgements

Acknowledgements anonymized for peer review. 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
